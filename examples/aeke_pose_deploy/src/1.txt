  // cv::Mat tmp;
  // cv::threshold(outputs, tmp, -0.999999, 0.999999, cv::THRESH_TRUNC);
  // tmp.convertTo(tmp, CV_32FC3);
  // tmp = (tmp + 1) * 127.5;
  // tmp.convertTo(tmp, CV_8UC3);


  // // 将图像保存为文件
  // cv::imwrite("8_anime.jpg", tmp);
  // exit(0);



  // // 后处理/Post-processing
  // detect_result_group_t detect_result_group;
  // std::vector<float> out_scales;
  // std::vector<int32_t> out_zps;
  // for (int i = 0; i < io_num.n_output; ++i)
  // {
  //     out_scales.push_back(output_attrs[i].scale);
  //     out_zps.push_back(output_attrs[i].zp);
  // }
  // post_process((int8_t *)outputs[0].buf, (int8_t *)outputs[1].buf, (int8_t
  // *)outputs[2].buf, height, width,
  //              box_conf_threshold, nms_threshold, pads, scale_w, scale_h,
  //              out_zps, out_scales, &detect_result_group);

  // // 绘制框体/Draw the box
  // char text[256];
  // for (int i = 0; i < detect_result_group.count; i++)
  // {
  //     detect_result_t *det_result = &(detect_result_group.results[i]);
  //     sprintf(text, "%s %.1f%%", det_result->name, det_result->prop * 100);
  //     // 打印预测物体的信息/Prints information about the predicted object
  //     // printf("%s @ (%d %d %d %d) %f\n", det_result->name,
  //     det_result->box.left, det_result->box.top,
  //     //        det_result->box.right, det_result->box.bottom,
  //     det_result->prop); int x1 = det_result->box.left; int y1 =
  //     det_result->box.top; int x2 = det_result->box.right; int y2 =
  //     det_result->box.bottom;
  //     // rectangle(orig_img, cv::Point(x1, y1), cv::Point(x2, y2),
  //     cv::Scalar(256, 0, 0, 256), 3);
  //     // putText(orig_img, text, cv::Point(x1, y1 + 12),
  //     cv::FONT_HERSHEY_SIMPLEX, 0.4, cv::Scalar(255, 255, 255));
  // }

  // cv::Mat tmp;
  // cv::threshold(outputs, tmp, -0.999999, 0.999999, cv::THRESH_TRUNC);
  // tmp.convertTo(tmp, CV_32FC3);
  // tmp = (tmp + 1) * 127.5;
  // tmp.convertTo(tmp, CV_8UC3);


  // // 将图像保存为文件
  // cv::imwrite("8_anime.jpg", tmp);
  // exit(0);



  // // 后处理/Post-processing
  // detect_result_group_t detect_result_group;
  // std::vector<float> out_scales;
  // std::vector<int32_t> out_zps;
  // for (int i = 0; i < io_num.n_output; ++i)
  // {
  //     out_scales.push_back(output_attrs[i].scale);
  //     out_zps.push_back(output_attrs[i].zp);
  // }
  // post_process((int8_t *)outputs[0].buf, (int8_t *)outputs[1].buf, (int8_t
  // *)outputs[2].buf, height, width,
  //              box_conf_threshold, nms_threshold, pads, scale_w, scale_h,
  //              out_zps, out_scales, &detect_result_group);

  // // 绘制框体/Draw the box
  // char text[256];
  // for (int i = 0; i < detect_result_group.count; i++)
  // {
  //     detect_result_t *det_result = &(detect_result_group.results[i]);
  //     sprintf(text, "%s %.1f%%", det_result->name, det_result->prop * 100);
  //     // 打印预测物体的信息/Prints information about the predicted object
  //     // printf("%s @ (%d %d %d %d) %f\n", det_result->name,
  //     det_result->box.left, det_result->box.top,
  //     //        det_result->box.right, det_result->box.bottom,
  //     det_result->prop); int x1 = det_result->box.left; int y1 =
  //     det_result->box.top; int x2 = det_result->box.right; int y2 =
  //     det_result->box.bottom;
  //     // rectangle(orig_img, cv::Point(x1, y1), cv::Point(x2, y2),
  //     cv::Scalar(256, 0, 0, 256), 3);
  //     // putText(orig_img, text, cv::Point(x1, y1 + 12),
  //     cv::FONT_HERSHEY_SIMPLEX, 0.4, cv::Scalar(255, 255, 255));
  // }

  // BOX_RECT pads;
  // memset(&pads, 0, sizeof(BOX_RECT));
  // cv::Size target_size(width, height);
  // cv::Mat resized_img(target_size.height, target_size.width, CV_8UC3);
  // // 计算缩放比例/Calculate the scaling ratio
  // float scale_w = (float)target_size.width / img.cols;
  // float scale_h = (float)target_size.height / img.rows;

  // // 图像缩放/Image scaling
  // if (img_width != width || img_height != height) {
  //   // // rga
  //   rga_buffer_t src;
  //   rga_buffer_t dst;
  //   memset(&src, 0, sizeof(src));
  //   memset(&dst, 0, sizeof(dst));
  //   ret = resize_rga(src, dst, img, resized_img, target_size);
  //   if (ret != 0) {
  //     fprintf(stderr, "resize with rga error\n");
  //   }

  //   // opencv
  //   // float min_scale = std::min(scale_w, scale_h);
  //   // scale_w = min_scale;
  //   // scale_h = min_scale;
  //   // letterbox(img, resized_img, pads, min_scale, target_size);

  //   inputs[0].buf = resized_img.data;
  // } else {
  //   inputs[0].buf = img.data;
  // }


   std::vector<PosePoint> pose_result;
  int8_t *simcc_x = (int8_t *)outputs[0].buf;
  int8_t *simcc_y = (int8_t *)outputs[1].buf;

  size_t lengthx = outputs[0].size / sizeof(int8_t);

  // 将指针和长度传递给std::vector的构造函数，创建新的向量
  std::vector<int8_t> simcc_x_int8(simcc_x, simcc_x + lengthx);

  size_t lengthy = outputs[1].size / sizeof(int8_t);

  // 将指针和长度传递给std::vector的构造函数，创建新的向量
  std::vector<int8_t> simcc_x_int8(simcc_y, simcc_y + lengthy);

  std::vector<float> simcc_x_result;
  std::vector<float> simcc_y_result;
  // 将 std::vector<int8_t> 类型的向量 simcc_x_int8 转换为 std::vector<float>
  // 类型的向量 simcc_x_result
  simcc_x_result.reserve(simcc_x_int8.size());
  for (int i = 0; i < simcc_x_int8.size(); i++) {
    float value = deqnt_affine_to_f32(simcc_x_int8[i], 0,
                                      0.01); // 假设 scale 为 0.01，零点偏移为 0
    simcc_x_result.push_back(value);
  }

  // 将 std::vector<int8_t> 类型的向量 simcc_y_int8 转换为 std::vector<float>
  // 类型的向量 simcc_y_result
  simcc_y_result.reserve(simcc_y_int8.size());
  for (int i = 0; i < simcc_y_int8.size(); i++) {
    float value = deqnt_affine_to_f32(simcc_y_int8[i], 0,
                                      0.01); // 假设 scale 为 0.01，零点偏移为 0
    simcc_y_result.push_back(value);
  }

  for (int i = 0; i < 17; ++i) {
    // find the maximum and maximum indexes in the value of each Extend_width
    // length

    auto x_biggest_iter = std::max_element(
        simcc_x_result + i * extend_width, simcc_x_result + i * extend_width + extend_width);
    int max_x_pos = std::distance(simcc_x_result + i * extend_width, x_biggest_iter);
    int pose_x = max_x_pos / 2;
    float score_x = *x_biggest_iter;

    // find the maximum and maximum indexes in the value of each exten_height
    // length

    auto y_biggest_iter =
        std::max_element(simcc_y_result + i * extend_height,
                         simcc_y_result + i * extend_height + extend_height);
    int max_y_pos = std::distance(simcc_y_result + i * extend_height, y_biggest_iter);
    int pose_y = max_y_pos / 2;
    float score_y = *y_biggest_iter;

    // float score = (score_x + score_y) / 2;
    float score = std::max(score_x, score_y);

    PosePoint temp_point;
    temp_point.x = (pose_x - offset[0]) * img_w / (input_w - 2 * offset[0]);
    temp_point.y = (pose_y - offset[1]) * img_h / (input_h - 2 * offset[1]);
    temp_point.score = score;

    pose_result.emplace_back(temp_point);
  }


 RTMPose pose;
  // std::vector<float> input_tensor = pose.preprocess(Info.orig_img);

  BOX_RECT pads;
  memset(&pads, 0, sizeof(BOX_RECT));
  cv::Size target_size(width, height);
  cv::Mat resized_img(target_size.height, target_size.width, CV_8UC3);
  // 计算缩放比例/Calculate the scaling ratio
  float scale_w = (float)target_size.width / img.cols;
  float scale_h = (float)target_size.height / img.rows;

  // 图像缩放/Image scaling
  if (img_width != width || img_height != height) {
    // // rga
    rga_buffer_t src;
    rga_buffer_t dst;
    memset(&src, 0, sizeof(src));
    memset(&dst, 0, sizeof(dst));
    ret = resize_rga(src, dst, img, resized_img, target_size);
    if (ret != 0) {
      fprintf(stderr, "resize with rga error\n");
    }

    // opencv
    // float min_scale = std::min(scale_w, scale_h);
    // scale_w = min_scale;
    // scale_h = min_scale;
    // letterbox(img, resized_img, pads, min_scale, target_size);

    inputs[0].buf = resized_img.data;
  } else {
    inputs[0].buf = img.data;
  }


  // cv::Mat rkYolov5s::infer(cv::Mat &orig_img)
FrameInfo rkYolov5s::infer(FrameInfo &Info) {
  std::lock_guard<std::mutex> lock(mtx);
  cv::Mat img = Info.orig_img;
  // cv::cvtColor(Info.orig_img, img, cv::COLOR_BGR2RGB);
  img_width = Info.width;
  img_height = Info.height;
  int INPUT_H2 = 256;
  int INPUT_W2 = 192;
  cv::Mat orig_img;
  img.copyTo(orig_img);
  int padw2, padh2;
  cv::Mat pr_img2 = preprocess_img(img, INPUT_W2, INPUT_H2, padw2, padh2);
  float data[3 * 256 * 192];
  float simccX[17 * 384];
  float simccY[17 * 512];
  for (int i = 0; i < INPUT_W2 * INPUT_H2; i++) {
    data[i] = pr_img2.at<cv::Vec3b>(i)[2] / 255.0;
    data[i + INPUT_W2 * INPUT_H2] = pr_img2.at<cv::Vec3b>(i)[1] / 255.0;
    data[i + 2 * INPUT_W2 * INPUT_H2] = pr_img2.at<cv::Vec3b>(i)[0] / 255.0;
  }

  inputs[0].buf = data;
  rknn_inputs_set(ctx, io_num.n_input, inputs);

  rknn_output outputs[io_num.n_output];
  memset(outputs, 0, sizeof(outputs));
  for (int i = 0; i < io_num.n_output; i++) {
    outputs[i].want_float = 1;
  }

  // 模型推理/Model inference
  ret = rknn_run(ctx, NULL);
  ret = rknn_outputs_get(ctx, io_num.n_output, outputs, NULL);
  // std::cout << "n_outputs = " << io_num.n_output << std::endl;
  // 后处理
  std::vector<float> out_scales;
  std::vector<int32_t> out_zps;
  for (int i = 0; i < io_num.n_output; ++i) {
    out_scales.push_back(output_attrs[i].scale);
    out_zps.push_back(output_attrs[i].zp);
  }
  float *pblob[2];
  for (int i = 0; i < io_num.n_output; ++i) {
    pblob[i] = (float *)outputs[i].buf;
    std::cout << "outputs[" << i << "].size = " << outputs[i].size << std::endl;
  }

  std::vector<PosePoint> pose_result;
  float *simcc_x = (float *)pblob[0];
  float *simcc_y = (float *)pblob[1];

  size_t lengthx = outputs[0].size / sizeof(float);
  std::vector<float> simcc_x_result(simcc_x, simcc_x + lengthx);

  size_t lengthy = outputs[1].size / sizeof(float);
  std::vector<float> simcc_y_result(simcc_y, simcc_y + lengthy);
  std::cout << "simcc_x_result length: " << simcc_x_result.size() << std::endl;
  std::cout << "simcc_y_result length: " << simcc_y_result.size() << std::endl;
  int extend_height = 512;
  int extend_width = 384;
  for (int i = 0; i < 17; ++i) {
    // find the maximum and maximum indexes in the value of each Extend_width
    // length
    auto x_biggest_iter = std::max_element(
        simcc_x_result.begin() + i * extend_width,
        simcc_x_result.begin() + i * extend_width + extend_width);
    int max_x_pos = std::distance(simcc_x_result.begin() + i * extend_width,
                                  x_biggest_iter);
    int x_index = max_x_pos / 2;
    float score_x = *x_biggest_iter;

    // find the maximum and maximum indexes in the value of each exten_height
    // length
    auto y_biggest_iter = std::max_element(
        simcc_y_result.begin() + i * extend_height,
        simcc_y_result.begin() + i * extend_height + extend_height);
    int max_y_pos = std::distance(simcc_y_result.begin() + i * extend_height,
                                  y_biggest_iter);
    int y_index = max_y_pos / 2;
    float score_y = *y_biggest_iter;

    float r_w = INPUT_W2 / (orig_img.cols * 1.0);
    float r_h = INPUT_H2 / (orig_img.rows * 1.0);
    if (r_h > r_w) {
      x_index = x_index / r_w;
      y_index = (y_index - (INPUT_H2 - r_w * orig_img.rows) / 2) / r_w;
    } else {

      x_index = (x_index - (INPUT_W2 - r_h * orig_img.cols) / 2) / r_h;
      y_index = y_index / r_h;
    }

    // float score = (score_x + score_y) / 2;
    float score = std::max(score_x, score_y);
    std::cout << "temp_point.x: " << x_index << std::endl;
    std::cout << "temp_point.y: " << y_index << std::endl;

    PosePoint temp_point;
    // temp_point.x = (pose_x - offset[0]) * img_w / (input_w - 2 * offset[0]);
    // temp_point.y = (pose_y - offset[1]) * img_h / (input_h - 2 * offset[1]);
    temp_point.x = x_index;
    temp_point.y = y_index;
    temp_point.score = score;
    std::cout << "temp_point.x: " << temp_point.x << std::endl;
    std::cout << "temp_point.y: " << temp_point.y << std::endl;
    std::cout << "temp_point.score: " << temp_point.score << std::endl;
    pose_result.emplace_back(temp_point);
  }

  // std::vector<PosePoint> pose_result = pose.postprocess(
  //     pblob, img_width, img_height, outputs[0].size, outputs[1].size);

  for (int i = 0; i < pose_result.size(); ++i) {
    cv::circle(
        img, cv::Point(pose_result[i].x , pose_result[i].y) ,
        1, cv::Scalar{0, 0, 255}, 10, cv::LINE_AA);
  }
  // std::vector<std::pair<int, int>> coco_17_joint_links = {
  //     {0, 1},   {0, 2},   {1, 3},   {2, 4},  {5, 7},  {7, 9},
  //     {6, 8},   {8, 10},  {5, 6},   {5, 11}, {6, 12}, {11, 12},
  //     {11, 13}, {13, 15}, {12, 14}, {14, 16}};
  // for (int i = 0; i < coco_17_joint_links.size(); ++i) {
  //   std::pair<int, int> joint_links = coco_17_joint_links[i];
  //   cv::line(img,
  //            cv::Point(pose_result[joint_links.first].x,
  //                      pose_result[joint_links.first].y),
  //            cv::Point(pose_result[joint_links.second].x,
  //                      pose_result[joint_links.second].y),
  //            cv::Scalar{0, 255, 0}, 2, cv::LINE_AA);
  // }
  cv::imwrite("out.jpg", img);

  ret = rknn_outputs_release(ctx, io_num.n_output, outputs);

  return Info;
}

  BOX_RECT pads;
  memset(&pads, 0, sizeof(BOX_RECT));
  cv::Size target_size(width, height);
  cv::Mat resized_img(target_size.height, target_size.width, CV_8UC3);
  // 计算缩放比例/Calculate the scaling ratio
  float scale_w = (float)target_size.width / img.cols;
  float scale_h = (float)target_size.height / img.rows;

  // 图像缩放/Image scaling
  if (img_width != width || img_height != height) {
    // // rga
    rga_buffer_t src;
    rga_buffer_t dst;
    memset(&src, 0, sizeof(src));
    memset(&dst, 0, sizeof(dst));
    ret = resize_rga(src, dst, img, resized_img, target_size);
    if (ret != 0) {
      fprintf(stderr, "resize with rga error\n");
    }

    // opencv
    // float min_scale = std::min(scale_w, scale_h);
    // scale_w = min_scale;
    // scale_h = min_scale;
    // letterbox(img, resized_img, pads, min_scale, target_size);

    inputs[0].buf = resized_img.data;
  } else {
    inputs[0].buf = img.data;
  }


  #include "postprocess.h"
#include "preprocess.h"
#include "rknn_api.h"
#include <cinttypes>
#include <cstdint>
#include <iostream>
#include <mutex>
#include <stdio.h>
#include <sys/time.h>
#include <fstream>

#include "opencv2/core/core.hpp"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/imgproc.hpp"

#include "coreNum.hpp"
#include "aeke_pose.hpp"

static void dump_tensor_attr(rknn_tensor_attr *attr) {
  std::string shape_str = attr->n_dims < 1 ? "" : std::to_string(attr->dims[0]);
  for (int i = 1; i < attr->n_dims; ++i) {
    shape_str += ", " + std::to_string(attr->dims[i]);
  }

  // printf("  index=%d, name=%s, n_dims=%d, dims=[%s], n_elems=%d, size=%d, "
  //        "w_stride = %d, size_with_stride=%d, fmt=%s, "
  //        "type=%s, qnt_type=%s, "
  //        "zp=%d, scale=%f\n",
  //        attr->index, attr->name, attr->n_dims, shape_str.c_str(),
  //        attr->n_elems, attr->size, attr->w_stride, attr->size_with_stride,
  //        get_format_string(attr->fmt), get_type_string(attr->type),
  //        get_qnt_type_string(attr->qnt_type), attr->zp, attr->scale);
}

static unsigned char *load_data(FILE *fp, size_t ofst, size_t sz) {
  unsigned char *data;
  int ret;

  data = NULL;

  if (NULL == fp) {
    return NULL;
  }

  ret = fseek(fp, ofst, SEEK_SET);
  if (ret != 0) {
    printf("blob seek failure.\n");
    return NULL;
  }

  data = (unsigned char *)malloc(sz);
  if (data == NULL) {
    printf("buffer malloc failure.\n");
    return NULL;
  }
  ret = fread(data, 1, sz, fp);
  return data;
}

static unsigned char *load_model(const char *filename, int *model_size) {
  FILE *fp;
  unsigned char *data;

  fp = fopen(filename, "rb");
  if (NULL == fp) {
    printf("Open file %s failed.\n", filename);
    return NULL;
  }

  fseek(fp, 0, SEEK_END);
  int size = ftell(fp);

  data = load_data(fp, 0, size);

  fclose(fp);

  *model_size = size;
  return data;
}

static int saveFloat(const char *file_name, float *output, int element_size) {
  FILE *fp;
  fp = fopen(file_name, "w");
  for (int i = 0; i < element_size; i++) {
    fprintf(fp, "%.6f\n", output[i]);
  }
  fclose(fp);
  return 0;
}

aekePose::aekePose(const std::string &model_path) {
  this->model_path = model_path;
  nms_threshold = NMS_THRESH;      // 默认的NMS阈值
  box_conf_threshold = BOX_THRESH; // 默认的置信度阈值
}

int aekePose::init(rknn_context *ctx_in, bool share_weight) {
  // printf("Loading model...\n");
  int model_data_size = 0;
  model_data = load_model(model_path.c_str(), &model_data_size);
  // 模型参数复用/Model parameter reuse
  if (share_weight == true)
    ret = rknn_dup_context(ctx_in, &ctx);
  else
    ret = rknn_init(&ctx, model_data, model_data_size, 0, NULL);
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }

  // 设置模型绑定的核心/Set the core of the model that needs to be bound
  rknn_core_mask core_mask;
  switch (get_core_num()) {
  case 0:
    core_mask = RKNN_NPU_CORE_0;
    break;
  case 1:
    core_mask = RKNN_NPU_CORE_1;
    break;
  case 2:
    core_mask = RKNN_NPU_CORE_2;
    break;
  }
  ret = rknn_set_core_mask(ctx, core_mask);
  if (ret < 0) {
    printf("rknn_init core error ret=%d\n", ret);
    return -1;
  }

  rknn_sdk_version version;
  ret = rknn_query(ctx, RKNN_QUERY_SDK_VERSION, &version,
                   sizeof(rknn_sdk_version));
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }
  // printf("sdk version: %s driver version: %s\n", version.api_version,
  //        version.drv_version);

  // 获取模型输入输出参数/Obtain the input and output parameters of the model
  ret = rknn_query(ctx, RKNN_QUERY_IN_OUT_NUM, &io_num, sizeof(io_num));
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }
  // printf("model input num: %d, output num: %d\n", io_num.n_input,
  //        io_num.n_output);

  // 设置输入参数/Set the input parameters
  input_attrs =
      (rknn_tensor_attr *)calloc(io_num.n_input, sizeof(rknn_tensor_attr));
  for (int i = 0; i < io_num.n_input; i++) {
    input_attrs[i].index = i;
    ret = rknn_query(ctx, RKNN_QUERY_INPUT_ATTR, &(input_attrs[i]),
                     sizeof(rknn_tensor_attr));
    if (ret < 0) {
      printf("rknn_init error ret=%d\n", ret);
      return -1;
    }
    dump_tensor_attr(&(input_attrs[i]));
  }

  // 设置输出参数/Set the output parameters
  output_attrs =
      (rknn_tensor_attr *)calloc(io_num.n_output, sizeof(rknn_tensor_attr));
  for (int i = 0; i < io_num.n_output; i++) {
    output_attrs[i].index = i;
    ret = rknn_query(ctx, RKNN_QUERY_OUTPUT_ATTR, &(output_attrs[i]),
                     sizeof(rknn_tensor_attr));
    dump_tensor_attr(&(output_attrs[i]));
  }

  if (input_attrs[0].fmt == RKNN_TENSOR_NCHW) {
    // printf("model is NCHW input fmt\n");
    channel = input_attrs[0].dims[1];
    height = input_attrs[0].dims[2];
    width = input_attrs[0].dims[3];
  } else {
    // printf("model is NHWC input fmt\n");
    height = input_attrs[0].dims[1];
    width = input_attrs[0].dims[2];
    channel = input_attrs[0].dims[3];
  }
  // printf("model input height=%d, width=%d, channel=%d\n", height, width,
  //        channel);

  memset(inputs, 0, sizeof(inputs));
  inputs[0].index = 0;
  inputs[0].type = RKNN_TENSOR_UINT8;
  inputs[0].size = width * height * channel;
  inputs[0].fmt = RKNN_TENSOR_NHWC;
  // inputs[0].pass_through = 0;

  return 0;
}

rknn_context *aekePose::get_pctx() { return &ctx; }

// cv::Mat aekePose::infer(cv::Mat &orig_img)
FrameInfo aekePose::infer(FrameInfo &Info) {
  std::lock_guard<std::mutex> lock(mtx);
  cv::Mat img = Info.orig_img;
  // cv::Mat input_img;
  // cv::cvtColor(img, input_img, cv::COLOR_BGR2RGB);
  img_width = Info.width;
  img_height = Info.height;
  
  cv::Mat orig_img;
  img.copyTo(orig_img);
  object_detect_result_list od_results;
  cv::Mat affine_transform_reverse;

  if(Info.alg_type == AlgoType::kRtmpose){
    printf("start pose\n");
    BOX_RECT box;
    box.top = 0;
    box.left = 0;
    box.bottom = img_height;
    box.right = img_width;
    std::pair<cv::Mat, cv::Mat> crop_result_pair =
        CropImageByDetectBox(orig_img, box);
    cv::Mat crop_mat = crop_result_pair.first;
    affine_transform_reverse = crop_result_pair.second;
    cv::imwrite("a.jpg",crop_mat);
    cv::Mat input_mat_copy_rgb;
    cv::cvtColor(crop_mat, input_mat_copy_rgb, cv::COLOR_BGR2RGB);
    inputs[0].buf = crop_mat.data;
  }else if(Info.alg_type == AlgoType::kYolox){
    printf("start det\n");
    // BOX_RECT pads;
    // memset(&pads, 0, sizeof(BOX_RECT));
    cv::Size target_size(width, height);
    cv::Mat resized_img(target_size.height, target_size.width, CV_8UC3);
    // 计算缩放比例/Calculate the scaling ratio
    float scale_w = (float)target_size.width / img.cols;
    float scale_h = (float)target_size.height / img.rows;
    // 图像缩放/Image scaling
    if (img_width != width || img_height != height) {
      // // rga
      rga_buffer_t src;
      rga_buffer_t dst;
      memset(&src, 0, sizeof(src));
      memset(&dst, 0, sizeof(dst));
      ret = resize_rga(src, dst, orig_img, resized_img, target_size);
      if (ret != 0) {
        fprintf(stderr, "resize with rga error\n");
      }

      // opencv
      // float min_scale = std::min(scale_w, scale_h);
      // scale_w = min_scale;
      // scale_h = min_scale;
      // letterbox(orig_img, resized_img, pads, min_scale, target_size);

      inputs[0].buf = resized_img.data;
    } else {
      inputs[0].buf = img.data;
    }
  }
  


  rknn_inputs_set(ctx, io_num.n_input, inputs);
  rknn_output outputs[io_num.n_output];
  memset(outputs, 0, sizeof(outputs));
  for (int i = 0; i < io_num.n_output; i++) {
    outputs[i].want_float = 1;
  }

  // 模型推理/Model inference
  ret = rknn_run(ctx, NULL);
  ret = rknn_outputs_get(ctx, io_num.n_output, outputs, NULL);

  // std::cout << "n_outputs = " << io_num.n_output << std::endl;
  // 后处理
  // std::vector<float> out_scales;
  // std::vector<int32_t> out_zps;
  // for (int i = 0; i < io_num.n_output; ++i) {
  //   out_scales.push_back(output_attrs[i].scale);
  //   out_zps.push_back(output_attrs[i].zp);
  // }

  float *simcc_x_result = (float *)outputs[0].buf;
  float *simcc_y_result = (float *)outputs[1].buf;

  int extend_width = 384;
  int extend_height = 512;
  Info.pose_result.clear();
  // std::vector<PosePoint> Info.pose_result;
  for (int i = 0; i < 17; ++i) {
    
    // find the maximum and maximum indexes in the value of each Extend_width
    // length
    auto x_biggest_iter =
        std::max_element(simcc_x_result + i * extend_width,
                         simcc_x_result + i * extend_width + extend_width);
    int max_x_pos =
        std::distance(simcc_x_result + i * extend_width, x_biggest_iter);
    int pose_x = max_x_pos / 2;
    float score_x = *x_biggest_iter;

    // find the maximum and maximum indexes in the value of each exten_height
    // length
    auto y_biggest_iter =
        std::max_element(simcc_y_result + i * extend_height,
                         simcc_y_result + i * extend_height + extend_height);
    int max_y_pos =
        std::distance(simcc_y_result + i * extend_height, y_biggest_iter);
    int pose_y = max_y_pos / 2;
    float score_y = *y_biggest_iter;

    // float score = (score_x + score_y) / 2;
    float score = std::max(score_x, score_y);

    PosePoint temp_point;
    temp_point.x = int(pose_x);
    temp_point.y = int(pose_y);
    temp_point.score = score;
    std::cout << "x: " << temp_point.x << std::endl;
    std::cout << "y: " << temp_point.y << std::endl;
    std::cout << "score: " << temp_point.score << std::endl;
    Info.pose_result.emplace_back(temp_point);
  }
  
  for (int i = 0; i < Info.pose_result.size(); ++i) {
    cv::Mat origin_point_Mat = cv::Mat::ones(3, 1, CV_64FC1);
    origin_point_Mat.at<double>(0, 0) = Info.pose_result[i].x;
    origin_point_Mat.at<double>(1, 0) = Info.pose_result[i].y;

    cv::Mat temp_result_mat = affine_transform_reverse * origin_point_Mat;

    Info.pose_result[i].x = temp_result_mat.at<double>(0, 0);
    Info.pose_result[i].y = temp_result_mat.at<double>(1, 0);

  }
  for (int i = 0; i < Info.pose_result.size(); ++i) {
    
    cv::circle(img, cv::Point(Info.pose_result[i].x, Info.pose_result[i].y), 1,
               cv::Scalar{0, 0, 255}, 5, cv::LINE_AA);
  }
  std::vector<std::pair<int, int>> coco_17_joint_links = {
      {0, 1},   {0, 2},   {1, 3},   {2, 4},  {5, 7},  {7, 9},
      {6, 8},   {8, 10},  {5, 6},   {5, 11}, {6, 12}, {11, 12},
      {11, 13}, {13, 15}, {12, 14}, {14, 16}};
  for (int i = 0; i < coco_17_joint_links.size(); ++i) {
    std::pair<int, int> joint_links = coco_17_joint_links[i];
    cv::line(img,
             cv::Point(Info.pose_result[joint_links.first].x,
                       Info.pose_result[joint_links.first].y),
             cv::Point(Info.pose_result[joint_links.second].x,
                       Info.pose_result[joint_links.second].y),
             cv::Scalar{0, 255, 0}, 2, cv::LINE_AA);
  }
  // cv::imwrite("out.jpg", img);

  ret = rknn_outputs_release(ctx, io_num.n_output, outputs);

  return Info;
}

aekePose::~aekePose() {
  deinitPostProcess();

  ret = rknn_destroy(ctx);

  if (model_data)
    free(model_data);

  if (input_attrs)
    free(input_attrs);
  if (output_attrs)
    free(output_attrs);
}
      for (int i = 0; i < frame.pose_result.size(); ++i) {
        cv::circle(src_img,
                   cv::Point(frame.pose_result[i].x, frame.pose_result[i].y), 1,
                   cv::Scalar{0, 0, 255}, 5, cv::LINE_AA);
        std::cout << "x: " << frame.pose_result[i].x << ", "
                  << "y: " << frame.pose_result[i].y << std::endl;
      }
      std::vector<std::pair<int, int>> coco_17_joint_links = {
          {0, 1},   {0, 2},   {1, 3},   {2, 4},  {5, 7},  {7, 9},
          {6, 8},   {8, 10},  {5, 6},   {5, 11}, {6, 12}, {11, 12},
          {11, 13}, {13, 15}, {12, 14}, {14, 16}};
      for (int i = 0; i < coco_17_joint_links.size(); ++i) {
        std::pair<int, int> joint_links = coco_17_joint_links[i];
        cv::line(src_img,
                 cv::Point(frame.pose_result[joint_links.first].x,
                           frame.pose_result[joint_links.first].y),
                 cv::Point(frame.pose_result[joint_links.second].x,
                           frame.pose_result[joint_links.second].y),
                 cv::Scalar{0, 255, 0}, 2, cv::LINE_AA);
RTM::RTM(const std::string &model_path) {
  this->model_path = model_path;
  nms_threshold = NMS_THRESH;      // 默认的NMS阈值
  box_conf_threshold = BOX_THRESH; // 默认的置信度阈值
}

int RTM::init(rknn_context *ctx_in, bool share_weight) {
  // printf("Loading model...\n");
  int model_data_size = 0;
  model_data = load_model(model_path.c_str(), &model_data_size);
  // 模型参数复用/Model parameter reuse
  if (share_weight == true)
    ret = rknn_dup_context(ctx_in, &ctx);
  else
    ret = rknn_init(&ctx, model_data, model_data_size, 0, NULL);
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }

  // 设置模型绑定的核心/Set the core of the model that needs to be bound
  rknn_core_mask core_mask;
  switch (get_core_num()) {
  case 0:
    core_mask = RKNN_NPU_CORE_0;
    break;
  case 1:
    core_mask = RKNN_NPU_CORE_1;
    break;
  case 2:
    core_mask = RKNN_NPU_CORE_2;
    break;
  }
  ret = rknn_set_core_mask(ctx, core_mask);
  if (ret < 0) {
    printf("rknn_init core error ret=%d\n", ret);
    return -1;
  }

  rknn_sdk_version version;
  ret = rknn_query(ctx, RKNN_QUERY_SDK_VERSION, &version,
                   sizeof(rknn_sdk_version));
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }
  // printf("sdk version: %s driver version: %s\n", version.api_version,
  //        version.drv_version);

  // 获取模型输入输出参数/Obtain the input and output parameters of the model
  ret = rknn_query(ctx, RKNN_QUERY_IN_OUT_NUM, &io_num, sizeof(io_num));
  if (ret < 0) {
    printf("rknn_init error ret=%d\n", ret);
    return -1;
  }
  // printf("model input num: %d, output num: %d\n", io_num.n_input,
  //        io_num.n_output);

  // 设置输入参数/Set the input parameters
  input_attrs =
      (rknn_tensor_attr *)calloc(io_num.n_input, sizeof(rknn_tensor_attr));
  for (int i = 0; i < io_num.n_input; i++) {
    input_attrs[i].index = i;
    ret = rknn_query(ctx, RKNN_QUERY_INPUT_ATTR, &(input_attrs[i]),
                     sizeof(rknn_tensor_attr));
    if (ret < 0) {
      printf("rknn_init error ret=%d\n", ret);
      return -1;
    }
    dump_tensor_attr(&(input_attrs[i]));
  }

  // 设置输出参数/Set the output parameters
  output_attrs =
      (rknn_tensor_attr *)calloc(io_num.n_output, sizeof(rknn_tensor_attr));
  for (int i = 0; i < io_num.n_output; i++) {
    output_attrs[i].index = i;
    ret = rknn_query(ctx, RKNN_QUERY_OUTPUT_ATTR, &(output_attrs[i]),
                     sizeof(rknn_tensor_attr));
    dump_tensor_attr(&(output_attrs[i]));
  }

  if (input_attrs[0].fmt == RKNN_TENSOR_NCHW) {
    // printf("model is NCHW input fmt\n");
    channel = input_attrs[0].dims[1];
    height = input_attrs[0].dims[2];
    width = input_attrs[0].dims[3];
  } else {
    // printf("model is NHWC input fmt\n");
    height = input_attrs[0].dims[1];
    width = input_attrs[0].dims[2];
    channel = input_attrs[0].dims[3];
  }
  // printf("model input height=%d, width=%d, channel=%d\n", height, width,
  //        channel);

  memset(inputs, 0, sizeof(inputs));
  inputs[0].index = 0;
  inputs[0].type = RKNN_TENSOR_UINT8;
  inputs[0].size = width * height * channel;
  inputs[0].fmt = RKNN_TENSOR_NHWC;
  inputs[0].pass_through = 0;

  return 0;
}

rknn_context *RTM::get_pctx() { return &ctx; }

// cv::Mat aekePose::infer(cv::Mat &orig_img)
FrameInfo RTM::infer(FrameInfo &Info) {
  std::lock_guard<std::mutex> lock(mtx);
  cv::Mat img = Info.orig_img;
  // cv::Mat input_img;
  // cv::cvtColor(img, input_img, cv::COLOR_BGR2RGB);
  img_width = Info.width;
  img_height = Info.height;
  int is_quan;
  letterbox_t letter_box;
  object_detect_result_list od_results;
  cv::Mat orig_img;
  cv::Mat affine_transform_reverse;
  img.copyTo(orig_img);

  if (Info.alg_type == AlgoType::kRtmpose) {
    is_quan = 1;
    // BOX_RECT box;
    // box.top = 0;
    // box.left = 0;
    // box.bottom = img_height;
    // box.right = img_width;
    std::pair<cv::Mat, cv::Mat> crop_result_pair =
        CropImageByDetectBox(orig_img, Info.DetectiontRects[Info.id]);
    cv::Mat crop_mat = crop_result_pair.first;
    affine_transform_reverse = crop_result_pair.second;

    cv::Mat input_mat_copy_rgb;
    cv::cvtColor(crop_mat, input_mat_copy_rgb, cv::COLOR_BGR2RGB);
    inputs[0].buf = input_mat_copy_rgb.data;

    rknn_inputs_set(ctx, io_num.n_input, inputs);
  } else if (Info.alg_type == AlgoType::kYolox) {

    is_quan = 1;
    BOX_RECT pads;
    memset(&pads, 0, sizeof(BOX_RECT));
    cv::Size target_size(width, height);
    cv::Mat resized_img(target_size.height, target_size.width, CV_8UC3);
    // 计算缩放比例/Calculate the scaling ratio
    float scale_w = (float)target_size.width / img.cols;
    float scale_h = (float)target_size.height / img.rows;

    // cv::cvtColor(img, orig_img, cv::COLOR_BGR2RGB);
    // 图像缩放/Image scaling
    if (img_width != width || img_height != height) {
      // // // rga
      // rga_buffer_t src;
      // rga_buffer_t dst;
      // memset(&src, 0, sizeof(src));
      // memset(&dst, 0, sizeof(dst));
      // ret = resize_rga(src, dst, orig_img, resized_img, target_size);
      // if (ret != 0) {
      //   fprintf(stderr, "resize with rga error\n");
      // }

      // opencv
      float min_scale = std::min(scale_w, scale_h);
      scale_w = min_scale;
      scale_h = min_scale;
      letterbox(orig_img, resized_img, pads, min_scale, target_size);
      letter_box.x_pad = pads.left;
      letter_box.y_pad = pads.top;
      letter_box.scale = min_scale;
      inputs[0].buf = resized_img.data;
      cv::Mat image(height, width, CV_8UC3, resized_img.data);
      cv::imwrite("resize_nano.jpg",image);
      rknn_inputs_set(ctx, io_num.n_input, inputs);
    } else {
      inputs[0].buf = img.data;
    }
  }

  rknn_output outputs[io_num.n_output];
  memset(outputs, 0, sizeof(outputs));
  for (int i = 0; i < io_num.n_output; i++) {
    outputs[i].want_float = is_quan;
  }
  // 模型推理/Model inference
  ret = rknn_run(ctx, NULL);
  ret = rknn_outputs_get(ctx, io_num.n_output, outputs, NULL);

  // std::cout << "n_outputs = " << io_num.n_output << std::endl;
  // 后处理
  std::vector<float> out_scales;
  std::vector<int32_t> out_zps;
  for (int i = 0; i < io_num.n_output; ++i) {
    out_scales.push_back(output_attrs[i].scale);
    out_zps.push_back(output_attrs[i].zp);
  }

  if (Info.alg_type == AlgoType::kRtmpose) {

    float *simcc_x_result = (float *)outputs[0].buf;
    float *simcc_y_result = (float *)outputs[1].buf;
    ret = rtm_post(simcc_x_result, simcc_y_result, affine_transform_reverse,
                   Info.pose_result);
  } else if (Info.alg_type == AlgoType::kYolox) {
    ret = yolox_nano_post_process(width, height, outputs, &letter_box,
                                  box_conf_threshold, nms_threshold,
                                  Info.DetectiontRects);
  }

  ret = rknn_outputs_release(ctx, io_num.n_output, outputs);

  return Info;
}

RTM::~RTM() {
  deinitPostProcess();

  ret = rknn_destroy(ctx);

  if (model_data)
    free(model_data);

  if (input_attrs)
    free(input_attrs);
  if (output_attrs)
    free(output_attrs);
}



  // frame.alg_type = AlgoType::kRtmpose;

  // int frames = 0;
  // auto beforeTime = startTime;
  // while (true) {
  //   frame.alg_type = AlgoType::kYolox;
  //   // frame = RTM.infer(frame);
  //   gettimeofday(&time, nullptr);
  //   auto time1 = time.tv_sec * 1000 + time.tv_usec / 1000;
  //   gettimeofday(&time, nullptr);
  //   auto time2 = time.tv_sec * 1000 + time.tv_usec / 1000;
  //   printf("det:%d\n", time2 - time1);

  //   if (RknnPool_.put(frame) != 0)
  //     break;

  //   if (frames >= threadNum && RknnPool_.get(frame) != 0)
  //     break;
  //   // 将帧放入队列
  //   {
  //     std::lock_guard<std::mutex> lock(mtx);
  //     frameQueue.push(frame);
  //   }

  //   gettimeofday(&time, nullptr);
  //   auto time3 = time.tv_sec * 1000 + time.tv_usec / 1000;
  //   printf("pose:%d\n", time3 - time2);
  //   frames++;
  //   // if(frames>1){break;}

  //   if (frames % 120 == 0) {
  //     gettimeofday(&time, nullptr);
  //     auto currentTime = time.tv_sec * 1000 + time.tv_usec / 1000;
  //     printf("120帧内平均帧率:\t %f fps/s\n",
  //            120.0 / float(currentTime - beforeTime) * 1000.0);
  //     beforeTime = currentTime;
  //     // break;
  //   }
  // }